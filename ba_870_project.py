# -*- coding: utf-8 -*-
"""BA-870_Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qfRJrIb-MIXUBoNzgrMiW6CEtq1slpYY

# ***Heuristic Scoring Model for Detecting Suspicious Insider Trades***

# Loading the Data
"""

import pandas as pd
import numpy as np
insiders = pd.read_csv('/content/WRDS_Insiders.csv')
insiders.head()

insiders.info()

"""## Data Description

wrds_filing_id: Unique identifier for the filing record.

issuer_id: Unique ID of the company (issuer) involved in the trade.

issuer_name: Name of the company whose insider made the trade.

issuer_trading_symbol: Stock ticker symbol of the issuing company.

issuer_sic: Standard Industrial Classification (SIC) code of the issuer's industry.

filing_date: Date the insider trade was officially filed with the SEC.

acceptance_date: Date when the SEC accepted the filing.

publication_date: Date the filing was publicly published or disclosed.

document_10b5one_flag: Indicates if the filing references a 10b5-1 prearranged trading plan.

reporting_owner_id: Unique identifier for the insider (owner) who made the trade.

reporting_owner_name: Name of the insider reporting the trade.

is_director: Flag indicating if the insider is a company director.

is_officer: Flag indicating if the insider holds an officer position.

officer_title: Title of the officer (e.g., CEO, CFO).

is_other: Flag for other types of insider roles not covered by director or officer.

other_text: Description of the insider’s role if marked as "other."

is_ten_percent_owner: Indicates if the insider owns ≥10% of company shares.

trans_date: Date when the insider trade transaction occurred.

trans_acquired_disposed_code: Code indicating whether shares were acquired (‘A’) or disposed (‘D’).

trans_code: Detailed transaction code explaining the nature of the trade.

trans_price_per_share: Price per share at which the trade was executed.

trans_shares: Number of shares involved in the transaction.

direct_or_indirect_ownership: Indicates whether the ownership is direct ('D') or indirect ('I').

equity_swap_involved: Flag indicating if an equity swap was part of the transaction.

trans_form_type: Type of form used for reporting the transaction (e.g., Form 4).

shares_owned_following_trans: Total shares owned by the insider after the transaction.

value_owned_following_trans: Dollar value of holdings post-transaction (missing in this dataset).

transaction_10b5one_flag: Indicates if this specific trade was made under a 10b5-1 plan.

## Cleaning the Data
"""

insiders.isna().sum()

threshold = 0.50 * len(insiders)
insiders.dropna(thresh=threshold, axis=1, inplace=True)

insiders.dropna(inplace=True)

insiders.info()

insiders

"""# Feature Engineering"""

def engineer_insider_features(insiders):
    df = insiders.copy()

    # Parse date columns
    df['trans_date'] = pd.to_datetime(df['trans_date'])
    df['filing_date'] = pd.to_datetime(df['filing_date'])
    df['publication_date'] = pd.to_datetime(df['publication_date'])

    # Transaction-related features
    df['trade_to_filing_days'] = (df['filing_date'] - df['trans_date']).dt.days
    df['trade_to_publication_days'] = (df['publication_date'] - df['trans_date']).dt.days
    df['trade_price_log'] = np.log1p(df['trans_price_per_share'])
    df['abs_trade_value'] = df['trans_price_per_share'] * df['trans_shares']

    # Directional features
    df['is_purchase'] = (df['trans_acquired_disposed_code'] == 'A').astype(int)
    df['is_sale'] = (df['trans_acquired_disposed_code'] == 'D').astype(int)

    # Insider seniority flags
    title_col = df['officer_title'].fillna('').str.upper()
    df['is_exec'] = ((df['is_officer'] == 1) | title_col.str.contains("EXEC|VP|CHIEF|CIO|CTO")).astype(int)
    df['is_high_level'] = title_col.str.contains("CEO|PRESIDENT|CHAIR|CFO").astype(int)

    # Encode ownership type
    df['ownership_type'] = df['direct_or_indirect_ownership'].map({'D': 0, 'I': 1})

    # Group transaction codes
    def group_trans_code(x):
        if x in ['S', 'M']: return 'sale'
        elif x in ['P', 'A']: return 'purchase'
        else: return 'other'
    df['transaction_code_group'] = df['trans_code'].apply(group_trans_code)

    # One-hot encoding for group
    df = pd.get_dummies(df, columns=['transaction_code_group'], prefix='trans')

    return df

"""## Engineered Features

trade_to_filing_days: Number of days between the trade and its official filing.

trade_to_publication_days: Number of days between the trade and its public disclosure.

trade_price_log: Log-transformed trade price per share to reduce skewness.

abs_trade_value: Total value of the trade (price per share × number of shares).

is_purchase: Binary flag indicating if the trade was an acquisition.

is_sale: Binary flag indicating if the trade was a disposal.

is_exec: Flag for whether the insider is an executive or holds a senior title.

is_high_level: Flag for if the insider holds a top-level executive role (CEO, CFO, etc.).

ownership_type: Encodes ownership type: 0 for direct, 1 for indirect.

transaction_code_group: Categorizes trade codes into 'purchase', 'sale', or 'other'.

trans_sale / trans_purchase / trans_other: One-hot encoded columns indicating trade type category.
"""

insiders_features = engineer_insider_features(insiders)
insiders_features

insiders_features.info()

"""# Building the Financial model

## Major Influencing Factors

Great question! Each component of your `suspicion_score` is carefully chosen to reflect behavioral or procedural red flags in insider trading. Here's how each factor contributes to identifying suspicious activity:

---

### 🔹 `0.30 * trade_size_pct` – **Trade Size Percentile**
Large trades are more likely to be motivated by material non-public information. By using the **percentile** of the trade size (rather than raw value), we capture how unusually large the trade is relative to others. This is important because outsized trades could imply higher risk or potential gains from insider information.

---

### 🔹 `0.25 * (1 - trade_to_publication_days / max)` – **Speed of Reporting**
SEC regulations require insiders to report trades quickly. The **longer the delay**, the more time an insider has to benefit before the trade becomes public, which is suspicious. Hence, this feature is **inversely scored**—faster reporting gets a higher (less suspicious) score, while delayed reporting is penalized.

---

### 🔹 `0.15 * is_high_level` – **Executive Status**
High-ranking executives (e.g., CEOs, CFOs) typically have more **privileged access** to sensitive information. Their trades are therefore more scrutinized. Giving weight to whether the insider is in a **high-level role** helps emphasize trades that are more likely to be information-driven.

---

### 🔹 `0.10 * (1 - transaction_10b5one_flag)` – **Lack of 10b5-1 Plan Protection**
Rule 10b5-1 trading plans provide legal protection when trades are **pre-scheduled**, reducing suspicion. A missing or unused 10b5-1 plan could suggest **more discretion or timing control**, which raises the risk. That’s why trades **without** this plan get a higher suspicious score.

---

### 🔹 `0.20 * past behavior (mean trade value percentile)` – **Insider's Past Behavior**
Past behavior is often predictive. If an insider **routinely makes large trades**, it could be a pattern worth attention. Ranking each insider’s **average past trade value** highlights those who consistently make aggressive or atypical moves. This captures **historical risk tendencies**.

## Logic Behind the Values

### 🔹 0.30 → **Trade Size Percentile**
This gets the highest weight because **large trades are often the most obvious signal** of unusual behavior. If an insider trades a huge amount of stock, especially if it's out of pattern for them, it could indicate material knowledge. Giving it 30% of the total score ensures that extreme trades have a big influence on the final classification.

---

### 🔹 0.25 → **Speed of Reporting**
Delayed reporting is a **procedural red flag**. If an insider is slow to report, they might be trying to avoid detection or time disclosures favorably. Giving this 25% of the score emphasizes its importance without overpowering trade size.

---

### 🔹 0.20 → **Past Behavior (Mean Insider Trade Value)**
This captures **patterns** in insider behavior. If someone consistently trades at high volumes, that might warrant more suspicion. It's a strong signal but not as directly impactful as a single large trade or delayed report, so it gets slightly less weight (20%).

---

### 🔹 0.15 → **Executive Status**
Executives (especially C-suite or Board members) have more access to material non-public info, but **not all exec trades are suspicious**—some are routine. That’s why it’s important, but it gets a moderate 15% weight.

---

### 🔹 0.10 → **10b5-1 Plan Not Used**
The presence of a Rule 10b5-1 plan **lowers suspicion** because it pre-commits trades in advance. If it's not used, the trade may be discretionary and therefore more suspicious. This is important, but the weakest among the signals, so it’s assigned the lowest weight (10%).

---

### 💡 Why These Numbers?
The weights sum to **1.0 (100%)**, which is important for normalization. They're chosen to reflect:

- **Impactfulness**: How much each factor should move the needle.
- **Interpretability**: Making it easy to explain to stakeholders.
- **Balance**: Avoiding dominance by a single feature (unless intentionally).

If we later gain access to labeled data, these weights can be **tuned or learned** using machine learning techniques. But for now, this setup gives you a practical, explainable framework.

## Model
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Assuming df = your filtered DataFrame
df = insiders_features.copy()

# 1. Trade Size Percentile
df['trade_size_percentile'] = df['abs_trade_value'].rank(pct=True)

# 2. Speed of Reporting Score (lower = more suspicious)
df['report_speed_score'] = 1 - MinMaxScaler().fit_transform(df[['trade_to_publication_days']])

# 3. Executive Position Score
df['exec_score'] = df[['is_exec', 'is_high_level']].sum(axis=1) / 2  # 0 to 1

# 4. Rule 10b5-1 Plan Status Score (if used = less suspicious)
df['10b5one_score'] = 1 - df['document_10b5one_flag']  # 0 if protected, 1 if not

# 5. Past Behavior Risk Score (insider's prior suspicious trades)
# Normalize inputs
df['trade_size_pct'] = df['abs_trade_value'].rank(pct=True)

# Updated scoring logic with balanced weights
df['suspicion_score'] = (
    0.30 * df['trade_size_pct'] +  # slightly reduced weight
    0.25 * (1 - df['trade_to_publication_days'] / df['trade_to_publication_days'].max()) +  # normalized speed
    0.15 * df['is_high_level'] +  # exec status
    0.10 * (1 - df['transaction_10b5one_flag']) +  # flag not used
    0.20 * df.groupby('reporting_owner_id')['abs_trade_value'].transform('mean').rank(pct=True)  # past behavior
)

# Normalize again
df['suspicion_score'] = (df['suspicion_score'] - df['suspicion_score'].min()) / (df['suspicion_score'].max() - df['suspicion_score'].min())

# Calibrated thresholds based on quantiles
low_thresh = df['suspicion_score'].quantile(0.33)
high_thresh = df['suspicion_score'].quantile(0.66)

def classify_suspicion(score):
    if score < low_thresh:
        return 'Routine'
    elif score < high_thresh:
        return 'Moderate'
    else:
        return 'Suspicious'

df['suspicion_level'] = df['suspicion_score'].apply(classify_suspicion)

# Show top suspicious trades
suspicious_trades = df.sort_values(by='suspicion_score', ascending=False).head(10)
print(suspicious_trades[['reporting_owner_name', 'trans_date', 'abs_trade_value', 'suspicion_score', 'suspicion_level']])

"""### Approach Used for Model

This code builds a **rule-based suspicion scoring system** to evaluate the likelihood that an insider trade might be suspicious, based on financial and behavioral patterns in the data. It first constructs individual features that reflect red flags: large trade size (`trade_size_percentile`), quick reporting after the trade (`report_speed_score`), the trader’s executive position (`exec_score`), whether they used a 10b5-1 trading plan (`10b5one_score`), and past behavior patterns derived from the average trade size of each insider. These factors are combined with specific weights into a single `suspicion_score`, reflecting a weighted risk indicator, which is then normalized between 0 and 1 for comparability.

To make the model interpretable, the score is mapped to qualitative labels — *Routine*, *Moderate*, or *Suspicious* — using threshold cutoffs at the 33rd and 66th percentiles. This unsupervised, heuristic approach doesn’t rely on labeled data but instead ranks behavior relative to peers and insider norms. The final output displays the top 10 most suspicious trades, providing a ranked risk view of insider behavior that can be used for audit, compliance, or further investigation.

### Advantage of Using Rule-Based Suspicion Scoring System

One major advantage of this insider trading scoring model is that it doesn't require labeled data, making it well-suited for real-world financial scenarios where confirmed cases of insider trading are rare or confidential. Instead, it uses a heuristic, unsupervised approach that evaluates trades based on risk signals such as trade size, reporting speed, executive role, use of a 10b5-1 plan, and past behavior. This allows firms to flag potentially suspicious activity without needing a history of confirmed fraud cases. Additionally, the model is lightweight and interpretable, which means it can be deployed in real-time to support compliance teams or internal auditors with fast, explainable alerts.

Another key advantage is its transparency and flexibility. Each component of the model is built on domain-specific logic, making it easy for financial analysts or regulators to understand and trust the reasoning behind each flag. The scoring framework can be fine-tuned or expanded — for instance, by adjusting weights or adding new features like market reaction or news sentiment — making it adaptable to changing regulatory landscapes or firm-specific compliance policies. This balance between simplicity, explainability, and adaptability makes the model a powerful foundation for insider trading risk assessment.

# Visualizing the results of the model

## Histogram of Suspicion Score
"""

import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid", palette="muted", font_scale=1.1)
plt.figure(figsize=(10, 6))
sns.histplot(df['suspicion_score'], bins=20, kde=True, color='crimson')
plt.title("Distribution of Suspicion Scores")
plt.xlabel("Suspicion Score")
plt.ylabel("Number of Trades")
plt.axvline(df['suspicion_score'].mean(), color='black', linestyle='--', label='Mean Score')
plt.legend()
plt.tight_layout()
plt.show()

"""### Analysis

The histogram shows a relatively uniform spread of suspicion scores across the 0 to 1 scale, with a slight right skew and a visible peak around 0.15–0.2. This suggests the model isn’t biased toward classifying trades as overly suspicious or overly benign — a key sign of balanced feature weighting. The presence of a well-defined mean (shown as a vertical dashed line) and a smooth KDE curve indicates that the model captures variance in trading behavior effectively. In simpler terms: your scoring system is producing a healthy distribution of values, not clumping everything toward one end, which would have made the model less useful in practice.

## Boxplot of Score by Suspicion Level
"""

plt.figure(figsize=(8, 6))
sns.boxplot(x='suspicion_level', y='suspicion_score', data=df, palette="coolwarm")
plt.title("Suspicion Score by Category")
plt.xlabel("Suspicion Level")
plt.ylabel("Score")
plt.tight_layout()
plt.show()

"""### Analysis

This boxplot clearly demonstrates that your classification thresholds (Routine < 0.4, Moderate < 0.7, Suspicious ≥ 0.7) align well with actual score distributions. The median scores in each category are well-separated, with very little overlap in the interquartile ranges — meaning trades classified as Suspicious consistently have higher scores than Moderate or Routine. This separation is crucial because it shows that the model’s score thresholds are meaningful and reflect real differences in trade characteristics. This validates that the scoring logic isn’t arbitrary — it's genuinely differentiating behavior types.

# Streamlit App
"""

# import streamlit as st

# st.set_page_config(page_title="Insider Trade Suspicion Scorer", layout="centered")

# st.title("📊 Insider Trade Suspicion Scorer")
# st.markdown("Enter trade details below to get a suspicion score and classification.")

# # User input fields
# trade_size = st.number_input("💰 Trade Value (in USD)", min_value=0.0, step=1000.0)
# reporting_delay = st.number_input("⏱ Days between Trade and Publication", min_value=0, step=1)
# is_high_level = st.selectbox("🏢 Is High-Level Executive?", ["No", "Yes"])
# used_10b5_1 = st.selectbox("📝 Was 10b5-1 Plan Used?", ["Yes", "No"])
# past_trade_avg = st.number_input("📈 Avg Past Trade Value (by this Insider)", min_value=0.0, step=1000.0)

# if st.button("🔍 Classify Trade"):
#     # --- Normalize components ---
#     size_pct = min(trade_size / 1_000_000, 1)  # Cap at $1M
#     speed_score = 1 - min(reporting_delay / 10, 1)  # Anything >10 days = 0
#     high_level_score = 1 if is_high_level == "Yes" else 0
#     plan_flag_score = 0 if used_10b5_1 == "Yes" else 1
#     past_avg_score = min(past_trade_avg / 1_000_000, 1)

#     # --- Final score calculation ---
#     score = round((
#         0.30 * size_pct +
#         0.25 * speed_score +
#         0.15 * high_level_score +
#         0.10 * plan_flag_score +
#         0.20 * past_avg_score
#     ), 2)

#     # --- Classification ---
#     if score < 0.4:
#         level = "✅ Routine"
#     elif score < 0.7:
#         level = "⚠️ Moderate"
#     else:
#         level = "🚨 Suspicious"

#     # --- Display results ---
#     st.markdown(f"### 🎯 Suspicion Score: **{score}**")
#     st.markdown(f"### Classification: **{level}**")
#     st.progress(score)

#     # --- Score breakdown ---
#     st.markdown("---")
#     st.markdown("#### 🔍 Explanation of Components")
#     st.markdown(f"""
#     - **Trade Size Percentile**: {size_pct:.2f}
#     - **Reporting Speed Score**: {speed_score:.2f}
#     - **Executive Level**: {high_level_score}
#     - **10b5-1 Plan Used**: {0 if plan_flag_score == 0 else 1}
#     - **Past Trade Behavior Score**: {past_avg_score:.2f}
#     """)

#     # --- Radar chart ---
#     labels = ['Trade Size', 'Reporting Speed', 'Exec Level', '10b5-1 Plan', 'Past Behavior']
#     values = [size_pct, speed_score, high_level_score, plan_flag_score, past_avg_score]
#     values += values[:1]  # Complete loop

#     angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
#     angles += angles[:1]

#     fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
#     ax.plot(angles, values, color='red', linewidth=2)
#     ax.fill(angles, values, color='red', alpha=0.25)
#     ax.set_xticks(angles[:-1])
#     ax.set_xticklabels(labels)
#     ax.set_yticklabels([])
#     ax.set_title("📊 Component Breakdown Radar", y=1.08)

#     st.pyplot(fig)

"""# Limitations

One limitation is the use of static, manually-assigned weights in the scoring formula, which, although informed by intuition and domain expertise, may not reflect real-world patterns or interactions in complex insider behavior. Without rigorous validation (e.g., backtesting against known enforcement actions), there’s a risk of both false positives and false negatives. The score also heavily relies on the availability and cleanliness of WRDS fields—missing or inconsistent data (like trans_price or 10b5-1 flags) can distort the outcome. Further, since there’s no machine learning component in the score calculation phase, the model doesn’t adapt or improve over time from patterns in labeled data (e.g., actual SEC investigations).

Another concern is that the behavioral risk captured is relative within a dataset snapshot—percentile ranks and quantile thresholds are dynamic and may shift significantly based on the chosen date range or subset of data, potentially undermining score stability across time or users. Moreover, the system assumes that all suspicious behavior can be captured by a small number of features, ignoring temporal context (e.g., clustering of trades around earnings announcements) or external variables (like macro news). Finally, the app's score does not explain causality—just correlation—which could be limiting when used in compliance or regulatory settings.

# Next Steps

To enhance this project, the next logical step is to introduce supervised learning—train a classifier (e.g., Random Forest or XGBoost) using historical trades labeled as investigated/not investigated or suspicious/non-suspicious (perhaps from SEC litigation archives). This would help in fine-tuning the feature weights and interactions more objectively. You could also integrate semi-supervised learning or anomaly detection (like Isolation Forest or Autoencoders) to better spot unseen or emerging patterns in behavior that your current rule-based model might miss. Cross-validation and ROC-AUC scoring would add rigor to model selection.

From a product and user perspective, you could build out dashboards to track insider scoring over time, flag spikes in risk across industries or firms, and add company-level aggregations. A natural extension is to tie insider trades to stock price movements—did “suspicious” trades precede abnormal returns? You could also add NLP processing to analyze textual disclosures or footnotes in the Form 4 filings. Lastly, integrate with an alert system or export APIs so analysts or compliance officers can subscribe to high-risk trades in real time. Adding explainability (SHAP or LIME) could also make the model more transparent and trusted by users.
"""